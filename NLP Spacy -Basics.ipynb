{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example use cases\n",
    "- Classify emails as Spam or Legitimate\n",
    "- Sentiment Analysis for Movie reviews or Product\n",
    "- Analyzing trends from written customers feedback\n",
    "- Understanding text commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Steps for working with Spacy \n",
    "- Loading the library\n",
    "- Building the pipeline object\n",
    "- using tokens\n",
    "- POS  (parts of speech) tagging\n",
    "- Understanding token attributes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  nlp() function from Spacy automatically takes raw text files and performs tokens,parse and describe the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spacy in the command Line using the below code\n",
    "pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en  import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x225c8e1e390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"The President’s SOTU Address is coming in the middle of his impeachment trial in the Senate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The The    Xxx True True\n",
      "President President    Xxxxx True False\n",
      "’s ’s    ’x False True\n",
      "SOTU SOTU    XXXX True False\n",
      "Address Address    Xxxxx True False\n",
      "is is    xx True True\n",
      "coming coming    xxxx True False\n",
      "in in    xx True True\n",
      "the the    xxx True True\n",
      "middle middle    xxxx True False\n",
      "of of    xx True True\n",
      "his his    xxx True True\n",
      "impeachment impeachment    xxxx True False\n",
      "trial trial    xxxx True False\n",
      "in in    xx True True\n",
      "the the    xxx True True\n",
      "Senate Senate    Xxxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDS TDS    XXX True False\n",
      "on on    xx True True\n",
      "Mutual Mutual    Xxxxx True False\n",
      "Funds Funds    Xxxxx True False\n",
      ": :    : False False\n",
      "I I    X True True\n",
      "- -    - False False\n",
      "T T    X True False\n",
      "Dept Dept    Xxxx True False\n",
      "clarifies clarifies    xxxx True False\n",
      "10 10    dd False False\n",
      "% %    % False False\n",
      "tax tax    xxx True False\n",
      "at at    xx True True\n",
      "source source    xxxx True False\n",
      "only only    xxxx True True\n",
      "on on    xx True True\n",
      "dividends dividends    xxxx True False\n",
      ", ,    , False False\n",
      "not not    xxx True True\n",
      "on on    xx True True\n",
      "capital capital    xxxx True False\n",
      "gains gains    xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"TDS on Mutual Funds: I-T Dept clarifies 10% tax at source only on dividends, not on capital gains\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2=nlp('it is better to give than receive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "better"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "give than receive"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'spacy.tokens.doc.Doc' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b4512156669b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'worthy'\u001b[0m \u001b[1;31m## Cannot rewrite into a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'spacy.tokens.doc.Doc' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "doc2[3]='worthy' ## Cannot rewrite into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=nlp('Ronaldo now has 50 Juventus goals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ronaldo | now | has | 50 | Juventus | goals | "
     ]
    }
   ],
   "source": [
    "##seperate the tokens \n",
    "for token in doc3:\n",
    "    print(token.text,end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in doc3.ents:\n",
    "    print(str(spacy,explain(entity.label_)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4=nlp('production loss is attributed to inefficiency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:\nhttps://spacy.io/usage/models",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-52f171c84242>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mdoc.pyx\u001b[0m in \u001b[0;36mnoun_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:\nhttps://spacy.io/usage/models"
     ]
    }
   ],
   "source": [
    "for chunk in doc4.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4=nlp('Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-3eb70048cc90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdisplacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'distance:110'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(docs, style, page, minify, jupyter, options, manual)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE096\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfactories\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mrenderer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmanual\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0m_html\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"parsed\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mminify\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\displacy\\render.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, options)\u001b[0m\n\u001b[0;32m     26\u001b[0m             color, bg, font)\n\u001b[0;32m     27\u001b[0m         \"\"\"\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compact\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_spacing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word_spacing\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m45\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrow_spacing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrow_spacing\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompact\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "displacy.render(doc4,jupyter=True,options={'distance:110'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data \t  \t 7370109823176849439 \t\n",
      "science \t  \t 15907409978124634296 \t\n",
      "is \t  \t 3411606890003347522 \t\n",
      "an \t  \t 15099054000809333061 \t\n",
      "inter \t  \t 11115815504542332625 \t\n",
      "- \t  \t 9153284864653046197 \t\n",
      "disciplinary \t  \t 15771201057713930352 \t\n",
      "field \t  \t 14195085135452951260 \t\n",
      "that \t  \t 4380130941430378203 \t\n",
      "uses \t  \t 15169213402950300534 \t\n",
      "scientific \t  \t 12096978277017718205 \t\n",
      "methods \t  \t 2524309096534353778 \t\n",
      ", \t  \t 2593208677638477497 \t\n",
      "processes \t  \t 2329187326147364143 \t\n",
      ", \t  \t 2593208677638477497 \t\n",
      "algorithms \t  \t 17871777411560780215 \t\n",
      "and \t  \t 2283656566040971221 \t\n",
      "systems \t  \t 2784036749983215141 \t\n",
      "to \t  \t 3791531372978436496 \t\n",
      "extract \t  \t 1199617523884050149 \t\n",
      "knowledge \t  \t 841909098595075712 \t\n",
      "and \t  \t 2283656566040971221 \t\n",
      "insights \t  \t 9238170900982487017 \t\n",
      "from \t  \t 7831658034963690409 \t\n",
      "structured \t  \t 18091975700715099219 \t\n",
      "and \t  \t 2283656566040971221 \t\n",
      "unstructured \t  \t 8720796447648573112 \t\n",
      "data \t  \t 6645506661261177361 \t\n",
      ". \t  \t 12646065887601541794 \t\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.lemma,'\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that appear so frequently that need not be identified as unique words,don't give any additionnal information and don't require through tagging are generally refered to as Stop words. This can be filtered from text for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'here', 'these', 'just', 'many', 'rather', 'throughout', 'fifty', 'anyhow', 'since', 'ca', 'nowhere', 'only', 'sometimes', 'much', 'seeming', 'empty', 'first', 'n‘t', '’ve', 'any', 'between', 'wherein', 'therefore', 'alone', 'sixty', 'hereupon', 'off', 'and', 'all', 'enough', 'anything', 'however', 'where', 'he', 'yourselves', 'whereby', 'him', 'say', 'made', 'might', 'would', 'call', 'whereupon', 'than', 'except', 'them', 'doing', 'with', 'could', 'their', 'during', 'twelve', 'everything', 'quite', 'themselves', '‘s', 'last', 'various', 'towards', 'well', 'beyond', \"'ll\", 'beside', 'nine', \"'m\", 'using', 'over', 'nothing', 'otherwise', 'nobody', 'amount', 'never', 'back', 'i', 'against', 'moreover', 'everyone', 'third', 'a', 'above', 'becoming', 'who', 'n’t', 'always', 'via', 'three', 'being', 'less', 're', 'out', 'ourselves', 'also', 'an', 'in', 'nor', 'keep', 'beforehand', 'latter', 'thence', 'after', 'name', 'own', 'now', '‘re', 'down', 'bottom', 'wherever', 'noone', 'almost', 'anyway', 'seems', 'unless', \"'d\", '’d', 'meanwhile', 'other', 'thereafter', 'across', 'even', 'was', 'to', 'same', 'namely', 'by', 'into', 'from', 'none', 'whom', 'everywhere', 'therein', 'though', 'until', 'its', 'through', 'serious', 'does', 'due', 'it', 'per', 'but', 'has', 'around', 'used', '‘ll', 'myself', 'without', 'anyone', 'you', 'put', 'whatever', 'she', 'full', 'whoever', 'perhaps', 'no', 'make', '‘m', 'already', 'yourself', 'side', 'which', 'they', 'amongst', 'ever', 'somewhere', 'becomes', 'take', 'her', '’s', \"'s\", 'four', 'so', 'often', 'whose', 'twenty', 'go', 'others', 'have', 'are', 'five', 'latterly', 'least', 'not', 'how', 'became', 'onto', 'former', 'nevertheless', 'most', 'while', 'had', 'too', 'at', 'hence', 'what', \"'ve\", 'for', 'some', 'whole', 'under', 'once', 'show', 'must', 'before', 'whence', 'hereby', 'whenever', 'another', 'herein', 'may', 'upon', 'whether', 'top', '‘d', 'do', '‘ve', 'every', 'neither', 'give', 'each', 'besides', 'both', 'such', 'few', 'yet', 'the', 'ours', 'regarding', 'something', 'that', 'cannot', 'itself', 'sometime', 'us', 'or', 'please', 'why', 'somehow', 'front', \"n't\", 'our', 'within', 'me', 'although', 'again', 'did', 'thereby', 'six', 'along', 'hers', 'yours', 'part', 'been', 'of', 'indeed', 'mine', 'whither', 'on', 'below', 'done', 'thereupon', 'two', \"'re\", 'can', 'fifteen', '’m', 'hundred', 'eight', 'seem', 'his', 'if', 'be', 'among', 'together', 'anywhere', 'then', 'eleven', 'is', 'either', 'about', 'will', 'elsewhere', 'there', 'this', 'those', 'we', 'move', 'behind', 'thus', 'next', 'himself', 'up', 'very', 'mostly', 'formerly', 'seemed', 'more', 'forty', 'whereas', 'herself', 'my', '’re', 'someone', 'whereafter', 'am', 'still', 'further', 'toward', 'one', 'afterwards', 'should', 'see', 'your', 'else', 'really', 'several', 'when', 'get', 'thru', 'were', 'become', '’ll', 'hereafter', 'ten', 'because', 'as'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words) ### retrive all the existing default stop words in the english dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## check if a word is s stop word\n",
    "print(nlp.vocab['with'].is_stop)\n",
    "print(nlp.vocab['hello'].is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add a new word to existing dictionary\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "nlp.vocab['btw'].is_stop=True\n",
    "len(nlp.Defaults.stop_words) ## length has incresed from 326 to 327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove  a new word to existing dictionary\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "nlp.vocab['beyond'].is_stop=False\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
